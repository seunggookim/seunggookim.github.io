<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content=""> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Musical emotion | Seung-Goo Kim </title> <meta name="author" content="Seung-Goo Kim"> <meta name="description" content="How do sounds evoke emotion?&lt;br&gt;Updated: 2024-09-23"> <meta name="keywords" content="brain, computation, music, emotion"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%B6&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://seunggookim.github.io/projects/proj_musical-emotion/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Seung-Goo</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Musical emotion</h1> <p class="post-description">How do sounds evoke emotion?<br>Updated: 2024-09-23</p> </header> <article> <h2 id="does-music-evoke-emotions">Does music evoke emotions?</h2> <p>While the need for verification never personally occurred to me (like.. of course?!), all comes from the fundamental problem of psychology: <em>We cannot directly observe others‚Äô subjective experiences.</em></p> <p>It would be unsurprising that some music does not ‚Äútell you anything‚Äù (i.e., it does not evoke any ‚Äúmeaningful‚Äù emotions other than boredom or disgust). But, more surprisingly, a systematic survey found that some people just don‚Äôt like music at all while these people were otherwise not different from those who enjoyed music <a href="https://doi.org/10.1525/mp.2013.31.2.118" rel="external nofollow noopener" target="_blank">(Mas-Herrero et al., 2013)</a>, which was followed by a large-scale survey and neural and physiological correlates. This particular disinterest in music was termed as ‚Äúmusical anhedonia‚Äù although whether this constitutes a clinical deficit to be listed in psychiatric manuals such as DSM-5 is unclear. Nonetheless, this evoked many studies on individual variability in musical reward sensitivity.</p> <h2 id="does-music-evoke-specific-emotions">Does music evoke specific emotions?</h2> <p>While acknowledging the great diversity of emotions evoked across individuals (and cultures) by an identical piece of music, it is still fascinating that such abstract sounds of music can evoke vivid and intense emotions. But do we experience similar emotions from an identical piece of music? If not, can we say it is music that evokes a certain emotion?</p> <p>Or, is this not how affective causality works? For example, an audio-visual stimulus of a smiling babyüë∂ (or a baby animalüê∂üê±) ‚Äúgenerally‚Äù evokes a joyful emotion in many people. But how reliable is this causation? There are other factors that modulate the evocation of joyful emotion. But this does not mean the causation (or association?) is completely random<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>.</p> <p>Now, here‚Äôs our hope: if there exists any kind of stable association between music and evoked emotions, we may model this as a (locally-)time-invariant linear system <a class="citation" href="#kim2022fn">(Kim, 2022)</a>. A preliminary result suggests abstract features extracted using a CNN could predict fMRI response time series in the medial prefrontal cortex, which could be more relevant to emotions than acoustic features <a class="citation" href="#kim2023icmpc">(Kim et al., 2023)</a>. Then, we can further investigate how stable this system is within an individual and how variable across individuals, cultures, and even clinical conditions.</p> <h2 id="havent-people-tried-and-failed-already">Haven‚Äôt people tried and failed already?</h2> <p>Even in the field of music information retrieval (MIR), musical emotion recognition has been a notorious task. An annual competition <a href="https://www.music-ir.org/mirex/wiki/MIREX_HOME" rel="external nofollow noopener" target="_blank">MIREX (Music Information Retrieval Evaluation eXchange)</a> keeps records of the state-of-the-art (SOTA) performance of various music-related tasks. Currently (2024-09-23), the <a href="https://www.music-ir.org/nema_out/mirex2020/results/act/mood_report/summary.html" rel="external nofollow noopener" target="_blank">latest result from MIREX2020 of the Audio Music Mood Classification</a> marks the highest classification accuracy of 0.6950 with the best combination of all the advanced deep neural networks. People wonder if this is due to the low reliability of the target labels of emotions.</p> <p>One possible new direction could be making use of physiological measures in addition to the self-reported ratings. This is what typically has been done in psychology labs on small scales. But with Apple Watch data, together with the Apple Music data, we may see interesting new results.</p> <h2 id="do-we-just-need-more-data">Do we just need more data?</h2> <p>Of course, ‚Äújust more data‚Äù won‚Äôt answer theoretical problems. However, we still don‚Äôt have empirical data with sufficient statistical power.</p> <p>What we can still do in an fMRI lab is to ‚Äúdensely sample‚Äù a selected group of participants. We have proposed a new dataset ‚ÄúManyMusicüé∂‚Äù <a class="citation" href="#kim2023cg">(Kim et al., 2023)</a> where we plan to collect fMRI scanning of 1,000 full-length creative-commons-licensed music listening. We will see what our dataset shows.</p> <h2 id="footnotes">Footnotes</h2> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Perhaps this may be even an absolutist confusion about empirical causation. An identical temperature may or may not boil the water depending on the pressure. Does it abolish the causal link between the temperature and the phase transition of the water?üôÑ¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">‚Ü©</a></p> </li> </ol> </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#c664ed"> <a href="https://www.icmpc.org/" rel="external nofollow noopener" target="_blank">ICMPC</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/kim_2023_icmpc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kim_2023_icmpc.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kim2023icmpc" class="col-sm-8"> <div class="title">Emotion-relevant Representations of Music Extracted by Convolutional Neural Networks Are Encoded in Medial Prefrontal Cortex</div> <div class="author"> <em>Seung-Goo Kim</em>,¬†<a href="https://people.duke.edu/~jto10/" rel="external nofollow noopener" target="_blank">Tobias Overath</a>,¬†and¬†<a href="https://www.aesthetics.mpg.de/en/the-institute/people/daniela-sammler.html" rel="external nofollow noopener" target="_blank">Daniela Sammler</a> </div> <div class="periodical"> <em>Proceedings ‚Äì The Joint Conference of the 17th International Conference on Music Perception and Cognition (ICMPC) and the 7th Conference of the Asia-Pacific Society for the Cognitive Sciences of Music (APSCOM)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/conferencepapers/Kim+.2023.musencemo.icmpc.final.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/talks/ICMPC.talk.2023-08-26_v2.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kim2023icmpc</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim, Seung-Goo and Overath, Tobias and Sammler, Daniela}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings -- The Joint Conference of the 17th International Conference on Music Perception and Cognition (ICMPC) and the 7th Conference of the Asia-Pacific Society for the Cognitive Sciences of Music (APSCOM)}</span><span class="p">,</span>
  <span class="na">date</span> <span class="p">=</span> <span class="s">{2023-08-01}</span><span class="p">,</span>
  <span class="na">date-modified</span> <span class="p">=</span> <span class="s">{2024-04-25 18:48:05 +0200}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{International Conference on Music Perception and Cognition (ICMPC)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Emotion-relevant Representations of Music Extracted by Convolutional Neural Networks Are Encoded in Medial Prefrontal Cortex}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CG</abbr> <figure> <picture> <img src="/assets/img/publication_preview/kim_2023_ce.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kim_2023_ce.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kim2023cg" class="col-sm-8"> <div class="title">Exploring affective experiences evoked by music: Study plan for a neurophysiological deep dataset "ManyMusicüé∂"</div> <div class="author"> <em>Seung-Goo Kim</em>,¬†Pablo Alonso-Jim√©nez,¬†Dmitry Dogdanov, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Xavier Serra, Daniela Sammler' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Proceedings ‚Äì CuttingGardens-Frankfurt: a distributed event of CuttingEEG</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/posters/Kim.2023.musafx_cuttingEEG_v2.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kim2023cg</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim, Seung-Goo and Alonso-Jim√©nez, Pablo and Dogdanov, Dmitry and Serra, Xavier and Sammler, Daniela}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proceedings -- CuttingGardens-Frankfurt: a distributed event of CuttingEEG}</span><span class="p">,</span>
  <span class="na">date</span> <span class="p">=</span> <span class="s">{2023-10-17}</span><span class="p">,</span>
  <span class="na">date-modified</span> <span class="p">=</span> <span class="s">{2024-04-25 18:48:05 +0200}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{cuttingEEG}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring affective experiences evoked by music: Study plan for a neurophysiological deep dataset "ManyMusicüé∂"}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#719e29"> <a href="https://www.frontiersin.org/" rel="external nofollow noopener" target="_blank">FNsci</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/kim_2022_fn.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="kim_2022_fn.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="kim2022fn" class="col-sm-8"> <div class="title">On the encoding of natural music in computational models and human brains</div> <div class="author"> <em>Seung-Goo Kim</em> </div> <div class="periodical"> <em>Frontiers in Neuroscience</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3389/fnins.2022.928841" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.928841" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="/assets/pdf/journalpapers/Kim.2022-09-20.FNsci.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=-M8Z3agAAAAJ&amp;citation_for_view=-M8Z3agAAAAJ:HDshCWvjkbEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-5-4285F4?logo=googlescholar&amp;labelColor=beige" alt="5 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p></p> <p>This article discusses recent developments and advances in the neuroscience of music to understand the nature of musical emotion. In particular, it highlights how system identification techniques and computational models of music have advanced our understanding of how the human brain processes the textures and structures of music and how the processed information evokes emotions. Musical models relate physical properties of stimuli to internal representations called features, and predictive models relate features to neural or behavioral responses and test their predictions against independent unseen data. The new frameworks do not require orthogonalized stimuli in controlled experiments to establish reproducible knowledge, which has opened up a new wave of naturalistic neuroscience. The current review focuses on how this trend has transformed the domain of the neuroscience of music.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kim2022fn</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kim, Seung-Goo}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3389/fnins.2022.928841}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1662-453X}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Frontiers in Neuroscience}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the encoding of natural music in computational models and human brains}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{16}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">bdsk-url-1</span> <span class="p">=</span> <span class="s">{https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.928841}</span><span class="p">,</span>
  <span class="na">bdsk-url-2</span> <span class="p">=</span> <span class="s">{https://doi.org/10.3389/fnins.2022.928841}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'seunggookim/seunggookim.github.io',
        'data-repo-id': '',
        'data-category': 'Comments',
        'data-category-id': '',
        'data-mapping': 'title',
        'data-strict': '1',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Seung-Goo Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme [v0.14.3]. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Image sources from <a href="https://www.irasutoya.com/" rel="external nofollow noopener" target="_blank">Irasutoya</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-3PBGLCQTTL"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-3PBGLCQTTL');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>