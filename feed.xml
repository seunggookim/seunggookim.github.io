<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://seunggookim.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://seunggookim.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-23T11:29:59+00:00</updated><id>https://seunggookim.github.io/feed.xml</id><title type="html">blank</title><subtitle>Computational Affective Neuroscience of Music </subtitle><entry><title type="html">TeaP2025 ended</title><link href="https://seunggookim.github.io/blog/2025/teap/" rel="alternate" type="text/html" title="TeaP2025 ended"/><published>2025-03-13T00:00:00+00:00</published><updated>2025-03-13T00:00:00+00:00</updated><id>https://seunggookim.github.io/blog/2025/teap</id><content type="html" xml:base="https://seunggookim.github.io/blog/2025/teap/"><![CDATA[<p>TeaP2025 (2025-03-09 to 2025-03-12, Frankfurt am Main, Germany) ended successfully. Here are some thoughts to share:</p> <h3 id="llm-based-conference-organization">LLM-based conference organization</h3> <p>The idea was: ‚ÄúCan we use the large-language model [LLM] embedding to measure semantic distance between conference abstracts?‚Äù ü§î</p> <h4 id="large-language-model">Large-language model?</h4> <p>You know, things like ChatGPT, Claude, Gemini. The things that parse strings of text tokens and generate fake text that sounds plausible. Maybe something similar to our ‚Äúlanguage center‚Äù in our brains does when we are extremely drunken. And also things that generate your fake homework essays and correct typos of this post.</p> <h4 id="embedding">Embedding?</h4> <p>It‚Äôs what non-philosophers think a ‚Äúrepresentation‚Äù would be like for an artificial neural network like a LLM. This is defined for a word, a sentenece, a paragraph, or a whole writing given as a set of 4000+ numbers (i.e., a vector) for an OpenAI model (<code class="language-plaintext highlighter-rouge">text-embedding-3-large</code>). See also: <a href="https://platform.openai.com/docs/api-reference/embeddings/create">https://platform.openai.com/docs/api-reference/embeddings/create</a></p> <h4 id="clustering-and-organizing">Clustering and organizing</h4> <p>Once you define the distance between abstracts, then you can perform clustering on that.</p> <p>We wanted to organize talk sessions while satisfying the following criteria:</p> <ul> <li> <p>Find near-optimal clustering with a maximaum of six abstracts per session.</p> </li> <li> <p>Once clusters are found, maximize distances between parallel sessions.</p> </li> </ul> <p>We wanted to organize poster sessions while considering these conditions:</p> <ul> <li> <p>Find near-optimal clustering while maintaining the number of clusters manageable for manual browsing on the conference web page (k &lt; 10).</p> </li> <li> <p>Ensure that poster presenters to view other posters on similar topics. To this end, the closest three posters are assigned to different days.</p> </li> <li> <p>Optimize attendees‚Äô walking paths by arranging posters so that their semantic similarity aligns with their physical proximity.</p> </li> </ul> <p><b>Want to know more?</b> <a href="/assets/pdf/posters/Kim.2025.emb_teap25_v3.pdf">[PDF]</a></p>]]></content><author><name></name></author><category term="nerd"/><summary type="html"><![CDATA[Helping organizing a conference]]></summary></entry><entry><title type="html">ChatGPT4o for BASH one-liners</title><link href="https://seunggookim.github.io/blog/2024/oneliner/" rel="alternate" type="text/html" title="ChatGPT4o for BASH one-liners"/><published>2024-09-15T00:00:00+00:00</published><updated>2024-09-15T00:00:00+00:00</updated><id>https://seunggookim.github.io/blog/2024/oneliner</id><content type="html" xml:base="https://seunggookim.github.io/blog/2024/oneliner/"><![CDATA[ <p>Apple and OpenAI recently made a deal, and now I can open ChatGPT window with Alt+Space on MacOS but still limited without a paid subscription. But still, it is a lot better than me in writing magical BASH commands, and the source of all evil‚Äìthe regular expression.</p> <p>I wanted to automatically update the list of journals in the <code class="language-plaintext highlighter-rouge">resume.json</code> file for this web page from the directories in my review folder. For this I need to find directories from the folder except for ones starting with <code class="language-plaintext highlighter-rouge">_</code>, select only the last names from the full path, then sort them ignoring cases, then replace a line break with a string of a comma and a space, then replace a place-holder <code class="language-plaintext highlighter-rouge">&lt;ADD_HERE&gt;</code> in a temporary file <code class="language-plaintext highlighter-rouge">resume-addhere.json</code> with this string, then save everything as a new <code class="language-plaintext highlighter-rouge">resume.json</code> after making a backup of the old <code class="language-plaintext highlighter-rouge">resume.json</code> file.</p> <p>So, this all can be done by this magic one-linerü™Ñ:</p> <pre><code class="language-{bash}">sed "s/\"&lt;ADD_HERE&gt;\": \"\"/\"summary\": \"`find ${MY_FOLDER_WHERE_I_PUT_MY_REVIEWS} -depth 1 -type d ! -name '_*' | awk -F/ '{print $NF}' | sort -f | sed -e ':a' -e 'N' -e '$!ba' -e 's/\n/, /g'`\"/g" resume-addhere.json &gt; resume.json
</code></pre> <p>And it <a href="/cv/">worked</a>!:satisfied:</p> <p>(But actually the ChatGPT took me a while since it hallucinated a bit about the sed command and was quite stubborn about it, then came up with a seemingly unnecessarily long option phrase‚Ä¶:see_no_evil:)</p> <p><em>UPDATED on 2024-09-16:</em> I‚Äôve got more stories on this now! This script worked fine on the local computer, but it doesn‚Äôt build and deploy the website on the remoteüôÉ. So I learned more about how to find error messages from the deployment process built in the workflows; and learned that all JSON files in the directory should be valid. So I made the place-holder a valid element:¬†<code class="language-plaintext highlighter-rouge">"&lt;ADD_HERE&gt;": ""</code>. The hard lesson was that I <em>must</em> obey the JSON format, all the time‚Ä¶</p>]]></content><author><name></name></author><category term="code"/><category term="nerd"/><summary type="html"><![CDATA[It's a magic!üò≥]]></summary></entry><entry><title type="html">LEAüëß</title><link href="https://seunggookim.github.io/blog/2024/lea/" rel="alternate" type="text/html" title="LEAüëß"/><published>2024-09-08T00:00:00+00:00</published><updated>2024-09-08T00:00:00+00:00</updated><id>https://seunggookim.github.io/blog/2024/lea</id><content type="html" xml:base="https://seunggookim.github.io/blog/2024/lea/"><![CDATA[ <table> <tbody> <tr> <td><img src="/assets/img/blog/lea-fig1.png" width="500px" alt="fig1" loading="eager" style="float" data-zoomable=""/></td> </tr> <tr> <td><em>Fig 1. LEA‚Äôs default plot.</em></td> </tr> </tbody> </table> <p>I‚Äôve created a MATLAB package called <a href="https://github.com/seunggookim/lea">LEAüëß</a> , which stands for Linearized Encoding Analysis. This was developed for my tutorial lecture at <a href="https://github.com/seunggookim/ksmpc-ss24-sess3">KSMPC-SS24</a>.</p> <p>While its functionality is limited, still one can use for most of behavioural and physiological time series data. Let me know if you‚Äôre interested in using this for your research. I am willing to further develop this for collaboration.</p>]]></content><author><name></name></author><category term="code"/><summary type="html"><![CDATA[Linearized Encoding Analysis MATLAB package]]></summary></entry><entry><title type="html">How do I edit a file in a Terminal? ü§∑</title><link href="https://seunggookim.github.io/blog/2024/clied/" rel="alternate" type="text/html" title="How do I edit a file in a Terminal? ü§∑"/><published>2024-08-21T00:00:00+00:00</published><updated>2024-08-21T00:00:00+00:00</updated><id>https://seunggookim.github.io/blog/2024/clied</id><content type="html" xml:base="https://seunggookim.github.io/blog/2024/clied/"><![CDATA[<p><img src="/assets/img/blog/clied-fig1.png" width="600px"/></p> <p>This is a light-hearted tutorial on <s>the holy Editor War</s> how to use a command-line interface (CLI) editors for beginners (who are unfamiliar with the Terminal), entitled ‚ÄúHow do I edit a file in a Terminal? ü§∑‚Äù, presented on the 21st of August, 2024 at MPIEA, Frankfurt am Main <a href="https://raw.githubusercontent.com/seunggookim/clied/8ecedbdd40e95e0340f1607bd578ea54fbc978d5/docs/2024-08-21_LabmeetingNCML_cli_Kim.pdf">[slides]</a> <a href="https://github.com/seunggookim/clied">[repo]</a></p> <p><em>UPDATE:</em> This is a part of <a href="/teaching/teach_mpi_computers/">a series of tutorials</a>.</p>]]></content><author><name></name></author><category term="nerd"/><summary type="html"><![CDATA['You cannot make a spoon that is better than a spoon' -Umbert Eco]]></summary></entry><entry><title type="html">Time deconvolution</title><link href="https://seunggookim.github.io/blog/2024/deconv/" rel="alternate" type="text/html" title="Time deconvolution"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://seunggookim.github.io/blog/2024/deconv</id><content type="html" xml:base="https://seunggookim.github.io/blog/2024/deconv/"><![CDATA[<p>Let us consider a linear model: \begin{equation} \label{eq:fir} \mathbf{y} = \mathbf{X} \mathbf{b} + \mathbf{e} \end{equation} where \(\mathbf{y} \in \Re^{T\,\times\,1}\) is a response time series over \(T\) timepoints, \(\mathbf{X} \in \Re^{T\,\times\,P}\) is a design matrix of \(P\) predictors such that \(\|\mathbf{X}'\mathbf{X}\| &gt; 0\), \(\mathbf{b} \in \Re^{P\,\times\,1}\) is an unknown weight vector, \(\mathbf{e} \in \Re^{T\,\times\,1}\) is a Gaussian noise vector \(\mathbf{e} \sim \mathcal{N} (\mathbf{0}, \sigma^2\mathbf{I})\).</p> <p>When the \(\mathbf{X}\) is a finite impulse respnose (FIR) design matrix (e.g., Toeplitz matrix), the linear model describes a convolution between one column in \(\mathbf{X}\) (e.g., \(\mathbf{x}^{(1)}\) where \(\)) and the vector \(\mathbf{b}\). Then, solving this model to find \(\mathbf{b}\) is equivalent to finding a deconvoultion from \(\mathbf{y}\) with \(\mathbf{X}\) to recover \(\mathbf{b}\).</p> <p>For that an FIR model is prone to overfitting to noise due to its flexibility, the stability of solution can be earned by introducing a small bias such as \(l^2\) penalty: \begin{equation} \mathbf{ {\hat b} } = (\mathbf{X}‚Äô \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{y} \end{equation} which is a well-known problem.</p> <p>In fMRI terminologies, \(\mathbf{b}\) is the HRF kernel for a given design vector \(\mathbf{x}^{(1)}\), which is assumed to be identical to underlying neural activity. Thus, without knowing the <em>true HRF</em>, estimating the <em>true neural activity</em> (\(\mathbf{x}^{(1)}\)) from \(\mathbf{y}\) is met with underdeterminedness. :shrug:</p>]]></content><author><name></name></author><category term="math"/><category term="fmri"/><summary type="html"><![CDATA[Is it a sample thing?]]></summary></entry><entry><title type="html">Temporal orders</title><link href="https://seunggookim.github.io/blog/2024/orders/" rel="alternate" type="text/html" title="Temporal orders"/><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://seunggookim.github.io/blog/2024/orders</id><content type="html" xml:base="https://seunggookim.github.io/blog/2024/orders/"><![CDATA[<p>Because of the sluggish (4-6 secs to peak) :snail: nature of BOLD (blood-oxygen-level-dependent) fMRI signal, the temporal orders of music, neural processing, and behavioral outcomes can be difficult to model. :see_no_evil:</p> <p>Consider a hypothetical case:</p> <table> <tbody> <tr> <td><img src="/assets/img/blog/orders-fig1.png" width="600px" alt="fig1" loading="eager" style="float" data-zoomable=""/></td> </tr> <tr> <td><em>Figure 1. A hypothetical case of temporal orders. MUSIC represents an impulse-like feature that evokes emotional response. NEURON shows a follow-up neuronal activity from 0.1 sec. BOLD signal is delayed by ~6 sec from the neural activity. BEHAVIOR-RED indicates a rapid response within 1-2 sec. BEHAVIOR-CYAN indicates a slow response even 8 seconds after the event.</em></td> </tr> </tbody> </table> <p><br/> We often assume an linear Granger causality from <code class="language-plaintext highlighter-rouge">MUSIC ‚Üí NEURON ‚Üí BEHAVIOR</code>. This may be too simplistic to model all kinds of responses, but it should do some of them.</p> <p>However, because the BOLD signal is a smoothed and delayed neural activity due to the slow heamodynamics (neural/glial activity uses up oxygen in the blood ‚Üí (perhaps) astrocytes control the blood vessel tension ‚Üí an influx of oxygenated blood), this linear Granger causaility can be broken: <code class="language-plaintext highlighter-rouge">MUSIC ‚Üí BEHAVIOR ‚Üí BOLD</code>. :confounded:</p> <p>Moreover, if a certain musical event is mediated by a neurochemical modulation, the affective response may take even longer than BOLD (i.e., BEHAVIOR-CYAN; or is this unrealistic?)</p> <p>In this case, using negative lags can be a solution? :shrug:</p> <hr/> <p><em>August 25, 2024</em></p> <p>But how do we know if the BEHAVIOR-CYAN is actually from this MUSIC event if this is via something that we don‚Äôt have data to model?</p> <p>Let us say in via some mysterious neurochemical mechanism, there exist a different neuron that is involved in this slow response:</p> <table> <tbody> <tr> <td><img src="/assets/img/blog/orders-fig2.png" width="700px" alt="fig1" loading="eager" style="float" data-zoomable=""/></td> </tr> <tr> <td><em>Figure 2. A hypothetical case of temporal orders with some more stuff. Perhaps for BEHAVIOR-CYAN, NEURON-PURPLE activity should be there and BOLD-ORANGE response can be seen.</em></td> </tr> </tbody> </table> <p><br/> Even without the BOLD delay, now we are discussing some magical process like this:magic_wand::</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>MUSIC ‚Üí NEURON1 ‚Üí BEHAVIOR1 ‚Üí (SOME MAGIC) ‚Üí NEURON2 ‚Üí BEHAVIOR2 
</code></pre></div></div> <p>But then, this cannot be shown in BOLD-GREEN but only in BOLD-ORANGE. :thinking:</p>]]></content><author><name></name></author><category term="fmri"/><category term="stat"/><summary type="html"><![CDATA[Twisted Granger causality]]></summary></entry><entry><title type="html">Projection matrix</title><link href="https://seunggookim.github.io/blog/2024/regproj/" rel="alternate" type="text/html" title="Projection matrix"/><published>2024-04-26T00:00:00+00:00</published><updated>2024-04-26T00:00:00+00:00</updated><id>https://seunggookim.github.io/blog/2024/regproj</id><content type="html" xml:base="https://seunggookim.github.io/blog/2024/regproj/"><![CDATA[<p>Let us consider a linear model: \begin{equation} \label{eq:fir} \mathbf{y} = \mathbf{X} \mathbf{b} + \mathbf{e} \end{equation} where \(\mathbf{y} \in \Re^{T\,\times\,1}\) is a response vector over \(T\) timepoints, \(\mathbf{X} \in \Re^{T\,\times\,P}\) is a design matrix of \(P\) predictors such that \(\|\mathbf{X}'\mathbf{X}\| &gt; 0\), \(\mathbf{b} \in \Re^{P\,\times\,1}\) is an unknown weight vector, \(\mathbf{e} \in \Re^{T\,\times\,1}\) is a Gaussian noise vector \(\mathbf{e} \sim \mathcal{N} (\mathbf{0}, \sigma^2\mathbf{I}_{T})\) where \(\sigma^2\) is a variance scalar and \(\mathbf{I}_T \in \Re^{T\,\times\,T}\) is an identity matrix.</p> <p>In a ridge regression and cross-validation over multiple partitions of data, if we use Pearson correlation for a prediction performance measure, the expectation of it can be simplified:</p> <p>\begin{equation} \label{eq:expcorr1} \mathbb{E}\left[\text{corr} \left( \hat{\mathbf y}_{2},\, \mathbf y_2 \right) \right] \propto \mathbf{s}_1‚Äô \mathbf{P}‚Äô \mathbf{s}_2 \end{equation}</p> <p>where \(\mathbf{s}\_i = \mathbf{X}\_i \mathbf{b}\) is a true signal in the \(i\)-th partition, and the first partition is <code class="language-plaintext highlighter-rouge">training</code> set and the second partition is <code class="language-plaintext highlighter-rouge">testing</code> set.</p> <p>The ridge projection matrix \(\mathbf P = \mathbf{X}_2 \mathbf{X}_1^{+} + \lambda \mathbf{I}\) can be somewhere between these two extreme cases depending on the optimal \(\lambda\):</p> \[\begin{cases} \mathbf{P} \approx \lambda \mathbf{I} &amp; \text{if } \lambda \gg \|\mathbf{X}_2 \mathbf{X}_1^{+}\| \\ \mathbf{P} = \mathbf{X}_2 \mathbf{X}_1^{+} &amp; \text{if } \lambda = 0 \end{cases}\] <p>where \((\cdot)^+\) denotes Moore‚ÄìPenrose inverse.</p> <p>Now, it shows for a higher regularization (i.e., a smaller signal), the projection matrix gets similar to an identity matrix, making the prediction accuracy highly dependent on the product of two true signals (\(\mathbf{s}_1'\mathbf{s}_2\)). :thinking: And this should be zero in a usual case.</p>]]></content><author><name></name></author><category term="math"/><category term="stat"/><summary type="html"><![CDATA[A little bit of mumbling]]></summary></entry></feed>